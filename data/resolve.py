import pickle

import faiss
import networkx as nx
import numpy as np
from sentence_transformers import SentenceTransformer
from tqdm import tqdm
from src.data_preprocessing.insert_custom_kg import create_custom_kg_for_batch


def build_graph_from_kgs(custom_kgs):
    G = nx.DiGraph()
    for kg in custom_kgs:
        for e in kg["entities"]:
            G.add_node(
                e["entity_name"],
                type=e["entity_type"],
                description=e["description"],
                source_id=e["source_id"]
            )
        for r in kg["relationships"]:
            if r["src_id"] and r["tgt_id"]:
                G.add_edge(
                    r["src_id"],
                    r["tgt_id"],
                    description=r["description"],
                    relation_type=r["keywords"],
                    weight=r["weight"]
                )
    print(f"ƒê·ªì th·ªã c√≥ {G.number_of_nodes()} node v√† {G.number_of_edges()} c·∫°nh.")
    return G


def build_faiss_index(csv_file, index_path="tiki_books.index", meta_path="tiki_books_meta.pkl"):
    print("ƒêang t·∫°o knowledge graph v√† chunk...")
    custom_kgs, df = create_custom_kg_for_batch(csv_file)

    # Build graph
    G = build_graph_from_kgs(custom_kgs)

    # L·∫•y t·∫•t c·∫£ c√°c chunk
    all_chunks = []
    for kg in custom_kgs:
        all_chunks.extend(kg["chunks"])
    print(f"T·ªïng s·ªë chunk: {len(all_chunks)}")

    # Kh·ªüi t·∫°o model embedding
    print("ƒêang t·∫°o embedding...")
    model = SentenceTransformer('all-MiniLM-L6-v2')

    embeddings = []
    for chunk in tqdm(all_chunks, desc="Encoding chunks"):
        emb = model.encode(chunk["content"])
        embeddings.append(emb)

    vectors = np.array(embeddings).astype("float32")

    # T·∫°o FAISS index
    dimension = vectors.shape[1]
    index = faiss.IndexFlatL2(dimension)
    index.add(vectors)

    # L∆∞u
    faiss.write_index(index, index_path)
    with open(meta_path, "wb") as f:
        pickle.dump({"chunks": all_chunks, "graph": G, "df": df}, f)

    print(f"ƒê√£ l∆∞u FAISS index t·∫°i: {index_path}")
    print(f"ƒê√£ l∆∞u metadata + graph t·∫°i: {meta_path}")

    return index, model, G, df


def query_with_graph(query: str, model, index, meta_path, k=5):
    with open(meta_path, "rb") as f:
        data = pickle.load(f)

    all_chunks = data["chunks"]
    G = data["graph"]
    df = data["df"]

    query_vector = model.encode([query]).astype("float32")
    distances, indices = index.search(query_vector, k)

    print(f"\n K·∫øt qu·∫£ truy v·∫•n: '{query}'\n")
    top_books = []
    for i, idx in enumerate(indices[0]):
        chunk = all_chunks[idx]
        print(f"Top {i + 1}:\n{chunk['content'][:300]}...\n{'-' * 80}")
        # Tr√≠ch t√™n s√°ch t·ª´ chunk
        lines = chunk["content"].split("\n")
        if lines:
            book_name = lines[0].replace("Book Name: ", "").strip()
            top_books.append(book_name)

    # M·ªü r·ªông context qua graph
    related_info = []
    for book in top_books:
        if book in G:
            neighbors = list(G.successors(book))
            for n in neighbors[:3]:  # ch·ªâ l·∫•y 3 quan h·ªá g·∫ßn nh·∫•t
                desc = G.nodes[n].get("description", "")
                related_info.append(f"{book} ‚Üí {n}: {desc}")

    if related_info:
        print("\nüìö C√°c m·ªëi quan h·ªá li√™n quan:")
        for rel in related_info:
            print("-", rel)

    return top_books, related_info


if __name__ == "__main__":
    csv_file = "C:/Users/Admin/PycharmProjects/LightRAG/data/crawl_tiki_data/books_data.csv"

    # T·∫°o FAISS index + Graph
    index, model, G, df = build_faiss_index(csv_file)

    # Truy v·∫•n th·ª≠
    query_with_graph("s√°ch c·ªßa Nguy·ªÖn Nh·∫≠t √Ånh v·ªÅ tu·ªïi h·ªçc tr√≤", model, index, "tiki_books_meta.pkl")

#
# File FAISS ƒë∆∞·ª£c t·∫°o: tiki_books.index
# File metadata ch·ª©a th√¥ng tin c√°c chunk: tiki_books_meta.pkl
