{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 02_faiss_index_benchmark.ipynb\n",
    "import faiss\n",
    "import numpy as np\n",
    "import time\n",
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "texts = [f\"Câu văn thử nghiệm số {i}\" for i in range(10000)]\n",
    "embeddings = np.array(model.encode(texts)).astype('float32')\n",
    "dim = embeddings.shape[1]\n",
    "\n",
    "# Các loại index để so sánh\n",
    "indexes = {\n",
    "    \"FlatL2\": faiss.IndexFlatL2(dim),\n",
    "    \"IVFFlat\": faiss.index_factory(dim, \"IVF100,Flat\"),\n",
    "    \"HNSW\": faiss.IndexHNSWFlat(dim, 32)\n",
    "}\n",
    "\n",
    "results = []\n",
    "query = model.encode([\"Câu văn thử nghiệm số 50\"]).astype(\"float32\")\n",
    "\n",
    "for name, index in indexes.items():\n",
    "    if \"IVF\" in name:\n",
    "        quantizer = faiss.IndexFlatL2(dim)\n",
    "        index = faiss.IndexIVFFlat(quantizer, dim, 100)\n",
    "        index.train(embeddings)\n",
    "    index.add(embeddings)\n",
    "\n",
    "    start = time.time()\n",
    "    distances, indices = index.search(query, 5)\n",
    "    latency = (time.time() - start) * 1000\n",
    "\n",
    "    results.append({\"Index\": name, \"Latency (ms)\": latency, \"Top Result ID\": indices[0][0]})\n",
    "\n",
    "pd.DataFrame(results)\n"
   ],
   "id": "de1bcb919969eb32"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\"\"\"\n",
    "Benchmark script: Nomic-embed-text vs SentenceTransformer(all-MiniLM-L6-v2) with FAISS\n",
    "\n",
    "Instructions:\n",
    "1. Prepare a dataset CSV/TSV with two columns: \"id\",\"text\" and another file \"queries.csv\" with \"id\",\"query\",\"relevant_id\" (one relevant id per query) or a relevance list.\n",
    "2. Create a virtualenv and install dependencies (internet required):\n",
    "   pip install sentence-transformers nomic-embed-text faiss-cpu tqdm numpy pandas scikit-learn\n",
    "   - If you have GPU and faiss-gpu, install faiss-gpu instead.\n",
    "3. Run:\n",
    "   python benchmark_nomic_vs_minilm_faiss.py --docs docs.csv --queries queries.csv --out results.json\n",
    "\n",
    "What the script does:\n",
    "- Loads documents and queries\n",
    "- Encodes documents and queries with both models\n",
    "- Builds FAISS index for each embedding type\n",
    "- Runs k-NN search and computes Recall@k, MRR, Mean latency per query\n",
    "- Saves results to JSON and prints a summary\n",
    "\n",
    "Notes:\n",
    "- nomic-embed-text API usage: imports from nomic\n",
    "- This script is written to be explicit and easy to adapt for custom datasets\n",
    "- If nomic-embed-text not available, the script will prompt and can fallback to other local models.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import argparse\n",
    "import time\n",
    "import json\n",
    "import os\n",
    "from typing import List, Tuple, Dict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Try imports; the user should install these packages in their environment\n",
    "try:\n",
    "    from sentence_transformers import SentenceTransformer\n",
    "except Exception as e:\n",
    "    SentenceTransformer = None\n",
    "\n",
    "try:\n",
    "    # nomic-embed-text exposes a model loader as `nomicbert` or through `nomic` package.\n",
    "    # The canonical import (as of 2024) is `from nomic import embed` or `from nomic.embedding import load_model`.\n",
    "    # We'll try common entrypoints.\n",
    "    from nomic import embed\n",
    "    def nomic_encode(texts: List[str], model_name: str = \"nomic-embed-text\") -> np.ndarray:\n",
    "        # embed.embed returns list of vectors\n",
    "        vs = embed.embed(texts, model=model_name)\n",
    "        return np.array(vs, dtype=np.float32)\n",
    "    nomic_available = True\n",
    "except Exception:\n",
    "    try:\n",
    "        # fallback: nomic-embed-text package\n",
    "        import nomic\n",
    "        from nomic.embedding import load_model\n",
    "        def nomic_encode(texts: List[str], model_name: str = \"nomic-embed-text-v1\") -> np.ndarray:\n",
    "            m = load_model(model_name)\n",
    "            vs = m.encode(texts)\n",
    "            return np.array(vs, dtype=np.float32)\n",
    "        nomic_available = True\n",
    "    except Exception:\n",
    "        nomic_available = False\n",
    "\n",
    "# FAISS import\n",
    "try:\n",
    "    import faiss\n",
    "except Exception:\n",
    "    faiss = None\n",
    "\n",
    "from sklearn.metrics import ndcg_score\n",
    "\n",
    "\n",
    "def load_docs(path: str) -> pd.DataFrame:\n",
    "    df = pd.read_csv(path)\n",
    "    assert 'id' in df.columns and 'text' in df.columns, \"docs csv must have columns 'id' and 'text'\"\n",
    "    return df\n",
    "\n",
    "\n",
    "def load_queries(path: str) -> pd.DataFrame:\n",
    "    q = pd.read_csv(path)\n",
    "    assert 'id' in q.columns and 'query' in q.columns and 'relevant_id' in q.columns, \"queries csv must have 'id','query','relevant_id'\"\n",
    "    return q\n",
    "\n",
    "\n",
    "def encode_texts(model_name: str, texts: List[str], model_type: str = 'minilm') -> np.ndarray:\n",
    "    \"\"\"Encode texts using chosen model type.\n",
    "    model_type: 'minilm' or 'nomic'\n",
    "    Returns numpy array float32\n",
    "    \"\"\"\n",
    "    if model_type == 'minilm':\n",
    "        if SentenceTransformer is None:\n",
    "            raise RuntimeError(\"sentence-transformers not installed\")\n",
    "        model = SentenceTransformer(model_name)\n",
    "        vectors = model.encode(texts, show_progress_bar=True, convert_to_numpy=True)\n",
    "        return vectors.astype(np.float32)\n",
    "    elif model_type == 'nomic':\n",
    "        if not nomic_available:\n",
    "            raise RuntimeError(\"nomic-embed-text not installed or not available\")\n",
    "        return nomic_encode(texts, model_name)\n",
    "    else:\n",
    "        raise ValueError('model_type must be minilm or nomic')\n",
    "\n",
    "\n",
    "def build_faiss_index(vectors: np.ndarray, index_type: str = 'Flat') -> faiss.Index:\n",
    "    dim = vectors.shape[1]\n",
    "    if index_type == 'Flat':\n",
    "        index = faiss.IndexFlatIP(dim)  # use Inner Product on normalized vectors\n",
    "    elif index_type == 'HNSW':\n",
    "        index = faiss.IndexHNSWFlat(dim, 32)\n",
    "    else:\n",
    "        raise ValueError('unknown index type')\n",
    "    return index\n",
    "\n",
    "\n",
    "def normalize_vectors(v: np.ndarray) -> np.ndarray:\n",
    "    norms = np.linalg.norm(v, axis=1, keepdims=True)\n",
    "    norms[norms == 0] = 1e-6\n",
    "    return v / norms\n",
    "\n",
    "\n",
    "def run_search(index: faiss.Index, query_vecs: np.ndarray, k: int = 10) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    # Ensure index and query use same metric: for IP we expect normalized vectors\n",
    "    D, I = index.search(query_vecs, k)\n",
    "    return I, D\n",
    "\n",
    "\n",
    "def compute_metrics(results_idx: np.ndarray, doc_ids: List, query_df: pd.DataFrame, topk: List[int] = [1,5,10]) -> Dict:\n",
    "    # results_idx: (nq, k) indices into doc vectors (0-based)\n",
    "    # doc_ids: list mapping index->doc_id\n",
    "    id_map = {doc_id: i for i, doc_id in enumerate(doc_ids)}\n",
    "    nq = results_idx.shape[0]\n",
    "    metrics = {}\n",
    "    for k in topk:\n",
    "        correct = 0\n",
    "        for qi in range(nq):\n",
    "            relevant = query_df.iloc[qi]['relevant_id']\n",
    "            topk_ids = [doc_ids[idx] for idx in results_idx[qi, :k]]\n",
    "            if relevant in topk_ids:\n",
    "                correct += 1\n",
    "        metrics[f'recall@{k}'] = correct / nq\n",
    "    # MRR\n",
    "    rr_sum = 0.0\n",
    "    for qi in range(nq):\n",
    "        relevant = query_df.iloc[qi]['relevant_id']\n",
    "        ranks = results_idx[qi]\n",
    "        rr = 0.0\n",
    "        for rank_pos, idx in enumerate(ranks, start=1):\n",
    "            if doc_ids[idx] == relevant:\n",
    "                rr = 1.0 / rank_pos\n",
    "                break\n",
    "        rr_sum += rr\n",
    "    metrics['MRR'] = rr_sum / nq\n",
    "    return metrics\n",
    "\n",
    "\n",
    "def benchmark(docs_path: str, queries_path: str, out_path: str, use_gpu: bool = False):\n",
    "    docs = load_docs(docs_path)\n",
    "    queries = load_queries(queries_path)\n",
    "\n",
    "    doc_texts = docs['text'].tolist()\n",
    "    doc_ids = docs['id'].tolist()\n",
    "    query_texts = queries['query'].tolist()\n",
    "\n",
    "    results = {}\n",
    "\n",
    "    # Models to evaluate\n",
    "    models = [\n",
    "        {'name': 'all-MiniLM-L6-v2', 'type': 'minilm'},\n",
    "        {'name': 'nomic-embed-text', 'type': 'nomic'}\n",
    "    ]\n",
    "\n",
    "    for m in models:\n",
    "        print(f\"\\n=== Encoding with {m['name']} ({m['type']}) ===\")\n",
    "        start = time.time()\n",
    "        try:\n",
    "            doc_vecs = encode_texts(m['name'], doc_texts, model_type=m['type'])\n",
    "            query_vecs = encode_texts(m['name'], query_texts, model_type=m['type'])\n",
    "        except Exception as e:\n",
    "            print(f\"Skipping model {m['name']} due to error: {e}\")\n",
    "            continue\n",
    "        encode_time = time.time() - start\n",
    "        print(f\"Encoding time: {encode_time:.2f}s\")\n",
    "\n",
    "        # Normalize\n",
    "        doc_vecs = normalize_vectors(doc_vecs)\n",
    "        query_vecs = normalize_vectors(query_vecs)\n",
    "\n",
    "        if faiss is None:\n",
    "            raise RuntimeError('faiss library not available. Install faiss-cpu or faiss-gpu')\n",
    "\n",
    "        # Build index\n",
    "        index = build_faiss_index(doc_vecs, index_type='Flat')\n",
    "        index.add(doc_vecs)\n",
    "\n",
    "        # Search\n",
    "        k = 10\n",
    "        t0 = time.time()\n",
    "        I, D = run_search(index, query_vecs, k=k)\n",
    "        search_time = time.time() - t0\n",
    "        avg_latency = search_time / len(query_texts)\n",
    "\n",
    "        metrics = compute_metrics(I, doc_ids, queries, topk=[1,5,10])\n",
    "        metrics.update({'encode_time_s': encode_time, 'search_time_s': search_time, 'avg_query_latency_s': avg_latency})\n",
    "\n",
    "        results[m['name']] = metrics\n",
    "        print(f\"Results for {m['name']}: {metrics}\")\n",
    "\n",
    "    # Save\n",
    "    with open(out_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(results, f, ensure_ascii=False, indent=2)\n",
    "    print(f\"Saved results to {out_path}\")\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--docs', required=True, help='CSV with columns id,text')\n",
    "    parser.add_argument('--queries', required=True, help='CSV with columns id,query,relevant_id')\n",
    "    parser.add_argument('--out', default='benchmark_results.json')\n",
    "    parser.add_argument('--gpu', action='store_true')\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    benchmark(args.docs, args.queries, args.out, use_gpu=args.gpu)\n"
   ],
   "id": "140e12fccffddb2d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-20T16:01:08.414263Z",
     "start_time": "2025-11-20T16:01:08.335265Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Nếu bạn đã có file gốc (vd: tiki_raw.csv) chứa các cột name,short_description,id...\n",
    "src = \"/srcc/example/faiss/data/books_data.csv\"  # đổi theo tên file gốc\n",
    "docs_out = \"docs.csv\"\n",
    "queries_out = \"queries.csv\"\n",
    "\n",
    "# đọc file gốc (nếu file là phần bạn gửi, bạn có thể copy paste vào tiki_raw.csv)\n",
    "df = pd.read_csv(src)\n",
    "\n",
    "# tạo cột text = name + \". \" + short_description (fillna để tránh NaN)\n",
    "df['short_description'] = df.get('short_description', df.get('short_description', \"\") ).fillna(\"\")\n",
    "df['text'] = df['name'].astype(str) + \". \" + df['short_description'].astype(str)\n",
    "\n",
    "# chỉ giữ id và text\n",
    "docs = df[['id', 'text']]\n",
    "docs.to_csv(docs_out, index=False, encoding='utf-8')\n",
    "\n",
    "# tạo queries mẫu (bạn có thể tự viết tốt hơn; ở đây là các query dựa trên tên)\n",
    "sample_queries = [\n",
    "    (\"Sách Walden sống một mình trong rừng\", 275861063),\n",
    "    (\"Tập Du ký Nam Phong tạp chí\", 274468056),\n",
    "    (\"Sách Vạn Dặm Đường Từ Một Bước Chân của Mavis\", 273842947),\n",
    "    (\"Sách Du hành cùng Herodotus\", 210277405),\n",
    "    (\"Sách Gỗ mun của Ryszard Kapuściński\", 210277378),\n",
    "    (\"Nhật ký sáu vạn dặm trên yên xe\", 204317934),\n",
    "    (\"Cơm nhà xứ Quảng sách\", 196902142),\n",
    "    (\"Sách Con Đường Tơ Lụa từ Pakistan tới Tây An\", 193209826),\n",
    "    (\"Du Ký Phan Quang Tiếc Nuối Hoa Hồng\", 192333128),\n",
    "    (\"Tác phẩm về Nam Phong tạp chí lịch sử\", 274468056),\n",
    "]\n",
    "\n",
    "queries_df = pd.DataFrame([\n",
    "    {\"id\": i+1, \"query\": q, \"relevant_id\": rid} for i, (q, rid) in enumerate(sample_queries)\n",
    "])\n",
    "queries_df.to_csv(queries_out, index=False, encoding='utf-8')\n",
    "\n",
    "print(\"Saved\", docs_out, \"and\", queries_out)\n"
   ],
   "id": "2035ef4424a538c1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved docs.csv and queries.csv\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "4f77adabfc084329"
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "nbformat": 5,
 "nbformat_minor": 9
}
